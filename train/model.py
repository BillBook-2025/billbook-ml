"""
ì–œ Transformerì— ë“¤ì–´ê°€ëŠ” ì…ë ¥ì´ ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ë¥¼ ë”í•´ì¤˜ì„œ....
í† í°ì˜ ìˆœì„œë¥¼ ëª¨ë¸ì´ ì•Œ ìˆ˜ ìˆê²Œ í•˜ëŠ” í´ë˜ìŠ¤ë˜
íŠ¸ëœìŠ¤í¬ë¨¸ ìì²´ëŠ” ìˆœì„œ ì •ë³´ë¥¼ ê°–ê³  ìˆì§€ ì•Šë‹¤ë„¤??

sin, cos í•¨ìˆ˜ë¥¼ í†µí•´ ê° ìœ„ì¹˜ë§ˆë‹¤ ê³ ìœ  ë²¡í„°ë¥¼ ìƒì„±í•¨
ì•„ ìš°ë¦¬ê°€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œì•„ì„œ ë§Œë“¤ì–´ì¤˜ì•¼í•˜êµ¬ë‚˜
"""


"""
ì¼ë‹¨ ì§€ê¸ˆ ëª¨ë¸ì—ì„ ... staticì„ transformerì— ì§ì ‘ì ìœ¼ë¡œ ë„£ì§€ ì•Šê³ 
ë”°ë¡œ ì €ì¥í•´ë†¨ë‹¤ê°€ ë§¨ ë§ˆì§€ë§‰ mlp ì¯¤ì— concatí•¨

âš™ï¸ í•˜ì§€ë§Œ ë³€í˜•ë„ ìˆìŒ

ë¬¼ë¡  ê²½ìš°ì— ë”°ë¼ì„œëŠ” staticì„ transformerì— â€œê°„ì ‘ì ìœ¼ë¡œâ€ ë„£ê¸°ë„ í•©ë‹ˆë‹¤.

ì˜ˆì‹œ 1: Staticì„ CLS í† í°ìœ¼ë¡œ ì‚½ì…

ğŸ‘‰ staticì„ [CLS] ê°™ì€ íŠ¹ë³„ í† í°ìœ¼ë¡œ ì‹œí€€ìŠ¤ ë§¨ ì•ì— ë¶™ì„

cls_token = self.static_proj(static).unsqueeze(1)  # (B, 1, emb_dim)
seq_emb = torch.cat([cls_token, seq_emb], dim=1)
seq_out = self.transformer(seq_emb, attn_mask=mask_with_cls)


ì´ë ‡ê²Œ í•˜ë©´ transformerê°€ static ì •ë³´ë„ attentionìœ¼ë¡œ ë³¼ ìˆ˜ ìˆì–´ìš”.
(ì´ê±´ â€œBERT-styleâ€ CTR ëª¨ë¸ì—ì„œ ìì£¼ ì”€)

ì˜ˆì‹œ 2: Staticì„ Attention Queryë¡œ ì‚¬ìš©

ğŸ‘‰ ì‹œí€€ìŠ¤ëŠ” Key/Value, staticì€ Queryë¡œ ì¨ì„œ
ì •ì  íŠ¹ì„± ê¸°ë°˜ìœ¼ë¡œ â€œsequence contextë¥¼ ìš”ì•½â€í•  ìˆ˜ë„ ìˆì–´ìš”.
"""



class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=512):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.register_buffer("pe", pe)  # (max_len, d_model)

    def forward(self, x):
        # x: (B, L, d_model)
        L = x.size(1)
        return x + self.pe[:L].unsqueeze(0)

class TabularSeqAttentionModel(nn.Module):
    """
    [ë²”ì£¼í˜• x_cat] â”€â–¶ [ê° Embedding â†’ concat â†’ cat_proj â†’ emb_dim]
    [ìˆ«ìí˜• x_num] â”€â–¶ [num_proj â†’ emb_dim]
                       â”‚
                       â–¼
              concat(cat_proj, num_proj) â”€â–¶ static_proj â†’ emb_dim
    
    [ì‹œí€€ìŠ¤ seq_items] â”€â–¶ [Embedding â†’ PosEnc â†’ Transformer â†’ pooled]
    [íƒ€ê²Ÿ target_item] â”€â–¶ [Embedding â†’ emb_dim]
    
    static(emb_dim) + target(emb_dim) + pooled(emb_dim)
        â”‚
        â–¼
      MLP â†’ logit
    """
    # ì‹¤ì œ ê³„ì‚°ì€ concat ë“±ë“±... forwardì—ì„œ!! initì—ì„  ê± êµ¬ì¡° ì •ì˜ë‘ ë³€ìˆ˜ í¬ê¸° ì •ë„..?
    def __init__(
        self,
        n_num,               # number of continuous numeric features -> numeric input layer í•„ìš”
        cat_cols,
        seq_vocab,           # vocab for sequence items (include +1 for padding index 0) -> seq emb layer ìƒì„±ìš©
        cat_vocabs,
        emb_dim=64,          # base embedding dim -> ëª¨ë“  emb ì°¨ì› í†µì¼ ê¸°ì¤€ê°’
        transformer_layers=2,
        transformer_heads=4,
        transformer_ff=256,
        seq_max_len=100,
        mlp_hidden=[512,256,128],
        dropout=0.2
    ):
        super().__init__()
        
        # --- embeddings for static categorical features ---
        self.cat_embs = nn.ModuleList([ # ê° ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ë”°ë¡œ ì„ë² ë”© í•´ì•¼í•¨
            nn.Embedding(
                num_embeddings=len(cat_vocabs[col]),
                # embedding_dim = min(emb_dim, max(4, (len(cat_vocabs[col])+1) // 10))
                # ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ì— ë”°ë¼ ì„ë² ë”© ì°¨ì› ì„¤ì •í•˜ëŠ”ê±´ë° ì´ê±° ë³´ë‹¤ ì•„ë˜ê»˜ ë” ì¢‹ëŒ€
                # min.. ì´ê±´ ë‹¨ìˆœíˆ 10ìœ¼ë¡œ ë‚˜ëˆ„ëŠ”ê±°ë¼ ì‘ì€ vocabì—ì„  ë„ˆë¬´ ì‘ê³  í° vocabì—ì„  ë¶€ì¡±í• ìˆ˜ë„ ìˆìŒ
                embedding_dim=int(min(600, round(1.6 * len(cat_vocabs[col]) ** 0.56)))
                # ì–œ ^0.56ë•ì— ì‘ì€ vocabì—” ì‘ê²Œ,, í°ê±°ì—” ì ì ˆíˆ í¬ê²Œ ë¹„ì„ í˜•ì ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤ëŠ”ë°?
            )
            for col in cat_cols
        ]) # ê²°ê³¼: (B, emb_i_dim)
        """
        ì¼ë‹¨ ì§€ê¸ˆì€ static ì¹´í…Œê³ ë¦¬ ì„ë² ë”© í¬ê¸°ê°€ ì œê°ê°ì„ ê·¼ë° ìˆ«ìí˜• í”¼ì²˜ëŠ” ë°”ë¡œ nn.Linear(n_num, emb_dim) ì¼ì¼€ í”„ë¡œì ì…˜í•¨!
        ìš°ë¦¬ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” seq embë˜ì§€ tgt emb ë˜ì§€ ëª¨ë‘ emb_dimë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ”ë° ì§€ê¸ˆ ìƒí™©ìœ¼ë¡  static ì„ë² ë”©ì´ë‘ emb_dimë‘ ë‹¤ë¦„
        + concatëœ static emb í¬ê¸°ê°€ ì œê°ê°ì´ë¼ í”¼ì³ë§ˆë‹¤ ì¤‘ìš”ë„ë„ ë‹¬ë¼ì§€ê³ 
        ++ ëª¨ë¸ì´ í•™ìŠµí•˜ë©´ì„œ ì •ì   featë‘ seq featë‘ ì¤‘ìš”ë„ êµ¬ë¬¸ì´ ë¶ˆëª…í™•í•´ì§
        ===> ì•„! ê·¸ëŸ¼ cat embë¥¼ emd_dimìœ¼ë¡œ projí•˜ë©´ ë˜êµ¬ë‚˜!
        """
        self.cat_proj = nn.Linear(
            sum(e.embedding_dim for e in self.cat_embs), # ê° cat embsë¥¼ concatí•˜ë©´ ì´ì •ë„ ê°’ì¸ê°€ë´
            emb_dim # ì–˜ë¡œ ì¶•ì†Œ
        )

        # projection for numeric features -> project to some latent dim
        # self.n_num = n_num
        if n_num > 0:
            self.num_proj = nn.Sequential(
                # nn.BatchNorm1d(n_num),
                # nn.LayerNorm(n_num), # ë§ˆì§€ë§‰ ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
                nn.Linear(n_num, emb_dim),
                nn.ReLU() # ì–˜ëŠ” ì™œ ë ë£¨ í•´ì£¼ëŠ”ê²¨?
                # -> ìˆ«ìí˜• ì…ë ¥ì€ scaleì´ ì—„ì²­ ì°¨ì´ë‚  ìˆ˜ë„ ìˆì–´ì„œ
                # ë¯¸ë¦¬ ReLUí•´ì„œ ì¢€ ê°’ ì¡°ì •í•˜ëŠ”ê±´ê°€ë´ (ê²½í—˜ì ì¸ê±´ê°€ë´„!)
            ) # ê²°ê³¼: (B, emb_dim)
        else:
            self.num_proj = None

        # --- sequence & target embeddings ---
        self.seq_item_emb = nn.Embedding(
            len(seq_vocab),
            emb_dim, # seqëŠ” í•˜ë‚˜ì˜ í° vocabë¼ì„œ í•œ ê°œì˜ ì„ë² ë”©ë§Œ ìˆìœ¼ë©´ ë¨
            padding_idx=0 # ê·¼ë° rowë§ˆë‹¤ ê¸¸ì´ ë‹¤ë¥¼í…Œë‹ˆ íŒ¨ë”©í•´ì£¼ì
        )
        self.target_emb = self.seq_item_emb # (B, L, emb_dim)

        # positional encoding ì•¤ ë¨¸ì§€ ê·¼ë°
        self.pos_enc = PositionalEncoding(emb_dim, max_len=seq_max_len)

        # transformer encoder
        layer = nn.TransformerEncoderLayer(
            d_model=emb_dim, 
            nhead=transformer_heads, 
            dim_feedforward=transformer_ff, 
            dropout=dropout, 
            batch_first=True # (B, L, D) í˜•íƒœ ìœ ì§€
        )
        self.transformer = nn.TransformerEncoder(layer, num_layers=transformer_layers)

        # MLP after concat (mlp ë„£ê¸° ì§ì „ì— concat í•˜ëŠ”ê±°ì„)
        # compute static embedding dim
        # ì•„ ì–˜ë¥¼ ì™œ í•˜ë‚˜ ì‹¶ì—ˆë„¤ ã„·ã„·ã„·ã„·ã„·ã„·
        # ì§€ê¸ˆ ëª¨ë¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ì•ˆì— static í”¼ì³ë¥¼ ì•ˆë„£ëŠ”ë‹¤ë„¤???~!~
        total_static_dim = (emb_dim if len(cat_cols) > 0 else 0) + (emb_dim if n_num > 0 else 0)
        # â† concat í›„ ìµœì¢… static feature dim (cat_proj + num_proj)
        self.static_proj = nn.Linear(total_static_dim, emb_dim)  # emb_dimìœ¼ë¡œ ì¶•ì†Œ
        self.static_proj_ln = nn.LayerNorm(emb_dim)
        self.static_proj_dropout = nn.Dropout(dropout)

        # --- final input dim for MLP ---
        final_input_dim = 3*emb_dim # static_dim + tgt_dim + pooled_dim (pooled_history?)
        self.layer_norm = nn.LayerNorm(final_input_dim)  # combined_dim = num + cat + seq feature dim
        
        mlp_layers = []
        inp = final_input_dim
        for h in mlp_hidden:
            mlp_layers += [nn.Linear(inp, h), nn.ReLU(), nn.Dropout(dropout)]
            inp = h
        mlp_layers += [nn.Linear(inp, 1)] # CTR ì˜ˆì¸¡ logit
        self.mlp = nn.Sequential(*mlp_layers)

    def forward(self, x_num, x_cat, seq_items, pad_mask, target_item, return_attn=False): # í´ë˜ìŠ¤ ì•ˆ í•¨ìˆ˜ë¼ì„œ ë§¨ í…€ì—  self ë„£ìŒ
        """
        x_num: (B, n_num) float or None
        x_cat: (B, n_cat) long or None
        seq_items: (B, L) long (padding idx=0)
        attn_mask: (B, L), True where padding
        target_item: (B,) long
        """
        B, L = seq_items.shape
        
        # --- static embeddings ---
        static_parts = []
        if x_cat is not None:
            cat_parts = []
            for i, emb in enumerate(self.cat_embs):
                if i < x_cat.shape[1]:  # x_catì˜ ë²”ìœ„ ì•ˆì—ì„œë§Œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì²´í¬
                    cat_parts.append(emb(x_cat[:, i]))  # ê° ë²”ì£¼í˜• íŠ¹ì„±ì— ëŒ€í•´ ì„ë² ë”©ì„ ì ìš©
            cat_embs = torch.cat(cat_parts, dim=1)
            static_parts.append(self.cat_proj(cat_embs))
        if x_num is not None and self.num_proj is not None:
            static_parts.append(self.num_proj(x_num))  # (B, emb_dim)
            
        if len(static_parts) > 0:
            static = torch.cat(static_parts, dim=1)
            static = F.layer_norm(static, static.shape[1:])
        else:
            static = torch.zeros(B, 0, device=seq_items.device)
        
        # --- sequence encoding with transformer ---
        seq_e = self.seq_item_emb(seq_items)  # (B, L, emb_dim) B,Lì„ ë„£ì—ˆìœ¼ë‹ˆ B,L,emb_dimì´ ë‚˜ì˜¤ì§€
        seq_e = self.pos_enc(seq_e)           # add positional info ìˆœì„œ ì •ë³´ ë¶€ì—¬

        # Transformer output: (B, L, emb_dim)
        seq_out = self.transformer(seq_e, src_key_padding_mask=pad_mask)

        # --- attention pooling using target embedding as query ---
        tgt = self.target_emb(target_item)  # (B, emb_dim)

        """
        Poolingì´ë€? ì—¬ëŸ¬ê°œì˜ ë²¡í„°ë¥¼ í•˜ë‚˜ë¼ í•©ì¹˜ëŠ” ì—°ì‚°!
        ex: ì‹œí€€ìŠ¤ ì„ë² ë”© (B,L,D) -> í‰ê· orí•© (B, D)
        í•˜ì§€ë§Œ í‰ê· ì´ë‚˜ í•©ì€ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ë˜‘ê°™ì´ ë´ì„œ ì¤‘ìš”í•œ í† í°ê³¼ ì¤‘ìš”í•˜ì§€ ì•Šì€ í† í°ì„ êµ¬ë¶„í•˜ì§€ ëª»í•¨
        
        Attention Poolingì€? ì‹œí€€ìŠ¤ì˜ ê° ë²¡í„°(seq_out)ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê³  í•©ì³ì„œ í•˜ë‚˜ë¡œ ë§Œë“¦
        ì´ ê°€ì¤‘ì¹˜ëŠ”... ì–´ë–¤ í† í°ì´ ì¤‘ìš”í•œì§€ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•´ì„œ ê²°ì •í•¨!
        """
        pooled, attn_w = self.attention_pooling(seq_out, tgt, pad_mask)
        
        # --- combine: static + target_emb + pooled_history ---
        # ìµœì¢… mlp ì „ì— concat
        static_fused = self.static_proj(static)
        static_fused = self.static_proj_ln(static_fused)
        static = self.static_proj_dropout(static_fused)
        combined = torch.cat([static, tgt, pooled], dim=1)
        # combined = self.layer_norm(combined)  # LayerNorm ì ìš©
        logit = self.mlp(combined).squeeze(1)
        
        # return logit
        if return_attn:
            return logit, attn_w
        else:
            return logit


# ğŸ”¤ ê¸°ë³¸ì ì¸ í…ì„œ í‘œê¸° ê·œì¹™
# ê¸°í˜¸	ì˜ë¯¸	ì˜ˆì‹œ (ì§ê´€ì  ë¹„ìœ )
# B	Batch size (ë°°ì¹˜ í¬ê¸°)	í•œ ë²ˆì— í•™ìŠµì— ë“¤ì–´ê°€ëŠ” â€œë°ì´í„° ìƒ˜í”Œ ê°œìˆ˜â€
# L	Sequence Length (ì‹œí€€ìŠ¤ ê¸¸ì´)	í•œ ì‚¬ìš©ìì˜ â€œí–‰ë™ ê¸°ë¡ ê¸¸ì´â€ (ì˜ˆ: ë³¸ ìƒí’ˆ 20ê°œ â†’ L=20)
# D	Embedding Dimension (ì„ë² ë”© ì°¨ì›)	ê° item/featureë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„° í¬ê¸° (ì˜ˆ: 64ì°¨ì› ë²¡í„°)
# n_num	ìˆ«ìí˜• í”¼ì²˜ ê°œìˆ˜	ì˜ˆ: price, age, rating_count ë“± 3ê°œë©´ n_num=3
# n_cat	ë²”ì£¼í˜• í”¼ì²˜ ê°œìˆ˜	ì˜ˆ: gender, region, device_type ë“± 5ê°œë©´ n_cat=5
# ğŸ“˜ ì˜ˆì‹œë¡œ ë³´ëŠ” ì…ë ¥ í˜•íƒœ
# ë³€ìˆ˜	Shape	ì„¤ëª…
# x_num	(B, n_num)	ìˆ«ìí˜• í”¼ì²˜ë“¤ (ì˜ˆ: [ê°€ê²©, ë‚˜ì´, í‰ì ])
# x_cat	(B, n_cat)	ë²”ì£¼í˜• í”¼ì²˜ë“¤ (ì˜ˆ: [ì„±ë³„, ì§€ì—­, ê¸°ê¸°])
# seq_items	(B, L)	ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ (ì˜ˆ: [ì•„ì´í…œ1, ì•„ì´í…œ2, ..., íŒ¨ë”©])
# attn_mask	(B, L)	ì‹œí€€ìŠ¤ ì¤‘ padding ìœ„ì¹˜ë¥¼ Trueë¡œ í‘œì‹œ
# target_item	(B,)	ì´ë²ˆì— ì˜ˆì¸¡í•  â€œí˜„ì¬ íƒ€ê²Ÿ ì•„ì´í…œâ€
# ğŸ“ˆ ì˜ˆì‹œë¡œ ê°ê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸°

# ì˜ˆë¥¼ ë“¤ì–´, í•œ ë²ˆì˜ í•™ìŠµ ë°°ì¹˜(batch)ê°€ 3ëª… ì‚¬ìš©ì ë°ì´í„°ë¡œ êµ¬ì„±ë¼ ìˆë‹¤ê³  í•˜ì:

# ì‚¬ìš©ì	seq_items	target_item
# user1	[15, 92, 33, 0, 0]	12
# user2	[45, 8, 0, 0, 0]	23
# user3	[67, 14, 88, 22, 19]	88

# B = 3 (3ëª… ì‚¬ìš©ì)

# L = 5 (sequence ê¸¸ì´ 5 â€” padding í¬í•¨)

# ê° seq_itemì€ vocab index (ì •ìˆ˜)

# 0ì€ padding index
# â†’ maskë¡œ í‘œì‹œí•´ì•¼ í•¨ (True = padding ìë¦¬)

# ğŸ§  ëª¨ë¸ ë‚´ë¶€ì—ì„œ shape íë¦„

# Transformer ë“¤ì–´ê°€ê¸° ì „í›„ë¡œ í…ì„œ shapeì´ ê³„ì† ë³€í•¨:

# ë‹¨ê³„	í…ì„œëª…	shape	ì„¤ëª…
# ì…ë ¥	seq_items	(B, L)	ì•„ì´í…œ ì¸ë±ìŠ¤ë“¤
# ì„ë² ë”©	seq_item_emb(seq_items)	(B, L, D)	ê° ì•„ì´í…œì„ Dì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
# PosEnc ì¶”ê°€	seq_e + pe	(B, L, D)	ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
# Transformer ì¶œë ¥	seq_out	(B, L, D)	ì‹œí€€ìŠ¤ ë¬¸ë§¥ ë°˜ì˜ëœ ë²¡í„°
# Attention pooling	pooled	(B, D)	íƒ€ê²Ÿ ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½ëœ ì‹œí€€ìŠ¤
# Static í”¼ì²˜	static	(B, D)	ìˆ«ìí˜• + ë²”ì£¼í˜• í†µí•© í”¼ì²˜
# Target embedding	tgt	(B, D)	ì˜ˆì¸¡ íƒ€ê²Ÿì˜ ì„ë² ë”©
# MLP ì…ë ¥	[static, tgt, pooled] concat	(B, 3*D)	ëª¨ë“  ì •ë³´ë¥¼ í•©ì¹¨
# ì¶œë ¥	logit	(B,)	ê° ìƒ˜í”Œì˜ í´ë¦­ í™•ë¥  ë¡œì§“
    def attention_pooling(self, seq_out, tgt, pad_mask):
        """
        seq_out: (B, L, D) Transformer ì¶œë ¥
        tgt: (B, D) target embedding
        pad_mask: (B, L) True = padding
        """
        # ==== attention ê³µì‹ ê¸°ì–µë‚˜ì§€? QÂ·K^T / âˆšd â†’ softmax â†’ * V ê·¸ê±´ê°€ë´ ====
        # ======== ê±°ê¸°ì˜ Q -> tgt, (K, V) -> seq_outì´ ë§¡ì€ ê±°ì„ ========
        # seq_out: (B, L, D), tgt: (B, D)
        # tgt.unsqueeze: (B, 1, D) -> ê± seq_outê³¼ ì°¨ì› ë§ê²Œ í•˜ë ¤êµ¬
        scores = (seq_out * tgt.unsqueeze(1)).sum(-1) # ê° ì‹œí€€ìŠ¤ í† í°ê³¼ targetì˜ ìœ ì‚¬ë„ ì ìˆ˜ (B, L)
        # (seq_out * tgt.unsqueeze(1)).shape -> B, L, D!!! .sum(-1)í•˜ë©´ ë§ˆì§€ë§‰ ì°¨ì› Dë¥¼ ëª¨ë‘ ë”í•¨! (B, L)
        d = seq_out.size(-1) # ì´ê±° ê·¸ ë£¨íŠ¸ d ê¸°ì–µë‚¨? ì°¨ì› ì¤„ì—¬ì„œ gradient ì•ˆì •í™” í•˜ëŠ”ê±°
        scores = scores / math.sqrt(d) # (B, L) scaled dot-product, Transformer attention
        scores = scores.masked_fill(pad_mask, float("-inf")) # padding ë¶€ë¶„ì€ -infë¥¼ ì¨ì„œ softmaxì—ì„œ ê°€ì¤‘ì¹˜ê°€ 0 ë‚˜ì˜¤ê²Œ

        '''
        # Label-aware attention
        # ì´ê²Œ ì§€ê¸ˆ í´ë¦­ ë¹„ìœ¨ì´ ë‚®ìœ¼ë‹ˆê¹Œ.... attention ë ˆë²¨ì—ì„œë„ í´ë¦­ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ì„œ
        # í´ë¦­ì´ ì‹¤ì œë¡œ ë˜ì—ˆìœ¼ë©´ ê·¸ ìƒ˜í”Œì˜ í† í°ì„ ê°€ì¤‘í•˜ëŠ”ê±´ë°....
        # ì¼ë‹¨ ì–˜ë„ ë‚˜ì¤‘ì— ê³ ë ¤...
        if self.training and label is not None:
            pos_mask = (label == 1).float().unsqueeze(1)  # (B, 1)
            scale = 1 + 0.5 * pos_mask  # label==1ì´ë©´ scores 1.5ë°° ì¦ê°€
            scores = scores * scale
        '''
        
        attn_w = torch.softmax(scores, dim=1) # (B, L) -> ê° ì‹œí€€ìŠ¤ í† í°ì— ëŒ€í•œ ê°€ì¤‘ì¹˜
        pooled = torch.einsum("bl, bld -> bd", attn_w, seq_out) # (B, D) ìµœì¢…ì ìœ¼ë¡œ Vë¥¼ ê³±í•˜ê³  sumí•˜ëŠ” ê³¼ì •ì„
        # attn_wë¥¼ ê° ì‹œí€€ìŠ¤ í† í°(seq_out; V)ì— ê³±í•´ì„œ ê°€ì¤‘ í¸ê· ì„ êµ¬í•¨ -> target ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½ëœ ì‹œí€€ìŠ¤ í‘œí˜„
        # ê²°êµ­ â€œì´ ì‚¬ìš©ìì˜ ê³¼ê±° í–‰ë™ ì¤‘ ì–´ë–¤ ê²Œ í˜„ì¬ target itemê³¼ ê´€ë ¨ì´ ë†’ì€ê°€?â€ë¥¼ ì˜ë¯¸
        # ==============================================================
        return pooled, attn_w